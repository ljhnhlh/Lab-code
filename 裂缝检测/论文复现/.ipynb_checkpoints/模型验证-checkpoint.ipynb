{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检测图片\n",
    "\n",
    "## 第一步实验\n",
    "\n",
    "首先将一张图片分成n份27*27的图片，预测后print，查看是否仅有0，1（emm，论文好像是设置阈值，先输出看看\n",
    "\n",
    "第一步实验结果：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "class Crack(nn.Module):\n",
    "    def __init__(self, Crack_cfg):\n",
    "        super(Crack, self).__init__()\n",
    "        self.features = self._make_layers(Crack_cfg)\n",
    "        # linear layer\n",
    "#         self.classifier = nn.Linear(512, 10)\n",
    "        self.linear1 = nn.Linear(32*6*6,64)\n",
    "#         self.linear1 = nn.Sequential([nn.Linear((32*6*6,64),\n",
    "#                                  nn.Sigmoid())])\n",
    "        self.linear2 = nn.Linear(64,64)\n",
    "#         self.linear2 = nn.Sequential([nn.Linear((64,64),\n",
    "#                                  nn.Sigmoid())])\n",
    "        self.linear3 = nn.Linear(64,25)\n",
    "#         self.linear3 = nn.Sequential([nn.Linear((64,25),\n",
    "#                                  nn.Sigmoid())])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "#         print(out.size())\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.linear3(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        \"\"\"\n",
    "        cfg: a list define layers this layer contains\n",
    "            'M': MaxPool, number: Conv2d(out_channels=number) -> BN -> ReLU\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "            \n",
    "#         layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Crack_cfg = {\n",
    "    'Crack11':[16,16,'M',32,32,'M']\n",
    "}\n",
    "\n",
    "model_t = Crack(Crack_cfg['Crack11']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import pickle\n",
    "import torch\n",
    "# pickle.load = partial(pickle.load, encoding=\"latin1\")\n",
    "# pickle.Unpickler = partial(pickle.Unpickler, encoding=\"latin1\")\n",
    "# model = torch.load('D:\\\\大三下\\\\人工神经网络\\\\crack.pt', map_location=lambda storage, loc: storage, pickle_module=pickle)\n",
    "\n",
    "\n",
    "# model = torch.load('D:/大三下/人工神经网络/best.pt')\n",
    "model = torch.load('D:/大三下/人工神经网络/crack.pt')\n",
    "model_t.load_state_dict(model)\n",
    "\n",
    "model_t.eval()\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-86-8543f4d14f7a>, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-86-8543f4d14f7a>\"\u001b[1;36m, line \u001b[1;32m35\u001b[0m\n\u001b[1;33m    width_num\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    " 1.需要将图片reshape成,长和宽都是27的倍数\n",
    " 2.切割图片\n",
    " 3.将这批图片处理成4元tuple，正则化到[-1,1]区间\n",
    " 4.将图片用模型检验\n",
    " 5.print实验结果 \n",
    " 6.将tensor转换成图片\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from skimage import io, transform\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "test_trainsforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "def classify(img):\n",
    "    # 切分为imgNum^2份\n",
    "    width_num = img.shape[0]//(27)    \n",
    "    print(width_num)\n",
    "    height_num = img.shape[1]//(27)\n",
    "    print(height_num)\n",
    "    # 分割图片\n",
    "    img_roi = []\n",
    "    for i in range(width_num):  # [1]480*360==15*11---height\n",
    "        for j in range(height_num):  # [2]column-----------width\n",
    "            img_roi.append(img[(i * 27):((i + 1) * 27), (j * 27):((j + 1) * 27)])\n",
    "\n",
    "    # 转换图片并开始识别\n",
    "    t = img_roi[0]\n",
    "    t = to_pil(t)\n",
    "    t = test_trainsforms(t).float()\n",
    "    t = t.unsqueeze_(0)\n",
    "    # print(t.size())\n",
    "    for i in range(len(img_roi)):\n",
    "        if i != 0 :\n",
    "            te = img_roi[i]\n",
    "            te = to_pil(te)\n",
    "            te = test_trainsforms(te).float()\n",
    "            te = te.unsqueeze_(0)\n",
    "            t = torch.cat((t,te),0)\n",
    "            # print(t.size())\n",
    "    return predict_img(t)\n",
    "# \t# print(st)\n",
    "# \t# 对目标进行定位\n",
    "# \tloc = []\n",
    "# \tfor i in st:\n",
    "# \t    x = i % imgNum\n",
    "# \t    y = i // imgNum\n",
    "# \t    loc.append([x,y])\n",
    "# \t# print(loc)\n",
    "# \t# 在图片上标记目标\n",
    "# \tfor i in loc:\n",
    "# \t    cv2.rectangle(img, (i[0]*imw,i[1]*imh), (i[0]*imw+imw,i[1]*imh+imh), (255,0,0), 3)\n",
    "# \t# cv2.rectangle(img, 左上角, 右下角, （r，g，b）, 粗细（1，2，3，，）)\n",
    "# \t# 返回已画好的图片\n",
    "# \treturn img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img(inputs):\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model_t(inputs)\n",
    "        imgNum = inputs.size()[0]\n",
    "        _,preds = torch.max(outputs,1)\n",
    "        std = []\n",
    "        print(preds)\n",
    "        return outputs\n",
    "#         for j in range(inputs.size()[0]):\n",
    "#             # ax = plt.subplot(imgNum//2,2,j+1)\n",
    "#             # ax.axis('off')\n",
    "#             # ax.set_title('predicted: {}'.format(classname[preds[j]]))\n",
    "#             if preds[j] == 0:\n",
    "#                 std.append(j)\n",
    "#             # imshow(inputs.cpu().data[j])\n",
    "#         return std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.5,0.5,0.5])\n",
    "    std = np.array([0.5,0.5,0.5])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "#     plt.imshow(inp)\n",
    "#     if title is not None:\n",
    "#         plt.title(title)\n",
    "#     plt.pause(0.001)  # pause a bit so that plots are updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "torch.Size([187, 25])\n",
      "(187, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "file = \"I:/1裂缝检测/CrackForest-dataset/image/001.jpg\"\n",
    "\n",
    "img = io.imread(file)\n",
    "#     print(classify(img))\n",
    "inp = classify(img)\n",
    "print(inp.shape)\n",
    "inp = inp.numpy();\n",
    "inp = inp.reshape(187,5,5)\n",
    "print(inp.shape)\n",
    "\n",
    "#此处需要将其转化成灰度图\n",
    "    \n",
    "# mean = np.array([0.5,0.5,0.5,1])\n",
    "# std = np.array([0.5,0.5,0.5,0])\n",
    "# inp = std * inp[1] + mean\n",
    "# inp = np.clip(inp[1], 0, 1)\n",
    "    \n",
    "#     new_img_PIL = transforms.ToPILImage()(out).convert('RGB')\n",
    "#     new_img_PIL.show() # 处理后的PIL图片\n",
    "#     output = classify(img)\n",
    "#     print(output)\n",
    "#     imshow(output[1]);\n",
    "\n",
    "# \tfiles = os.listdir(path)\n",
    "# \tfile_paths=[]#构造一个存放图片的列表数据结构\n",
    "# \tfor file in files:\n",
    "# \t    file_path= path +\"\\\\\" + file\n",
    "# \t    file_paths.append(file_path)\n",
    "# \tst = '../testdata/img8/'\n",
    "# \tfor i in range(len(file_paths)):\n",
    "# \t\timg = io.imread(file_paths[i])\n",
    "# \t\timg = classify(img,imgNum) # 将图片分成imgNum*imgNum份进行识别\n",
    "# \t\tplt.imshow(img)\n",
    "# \t\timg_name = st + str(i) + '.jpg'\n",
    "# \t\tprint(img_name)\n",
    "# \t\tcv2.imwrite(img_name,img)\n",
    "# \t\tif(i>100):\n",
    "# \t\t\tbreak;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 4675 into shape (425,275)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-7acd6a8476df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m480\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m27\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m320\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m27\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 4675 into shape (425,275)"
     ]
    }
   ],
   "source": [
    "t = np.array(inp[1])\n",
    "t[t > 0] = 1\n",
    "t[t<=0] = 0\n",
    "t = inp.reshape(480//27*25,320//27*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b60f741f28>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(t, cmap=plt.cm.gray, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
