{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检测图片\n",
    "\n",
    "## 第一步实验\n",
    "\n",
    "首先将一张图片分成n份27*27的图片，预测后print，查看是否仅有0，1（emm，论文好像是设置阈值，先输出看看\n",
    "\n",
    "第一步实验结果：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "class Crack(nn.Module):\n",
    "    def __init__(self, Crack_cfg):\n",
    "        super(Crack, self).__init__()\n",
    "        self.features = self._make_layers(Crack_cfg)\n",
    "        # linear layer\n",
    "#         self.classifier = nn.Linear(512, 10)\n",
    "#         self.linear1 = nn.Linear(32*6*6,64)\n",
    "#         self.linear2 = nn.Linear(64,64)\n",
    "#         self.linear3 = nn.Linear(64,25)\n",
    "        self.classifier = self.make_classifier()\n",
    "    \n",
    "    def make_classifier(self):\n",
    "        classifier = []\n",
    "        classifier += [nn.Linear(32*6*6,64),nn.ReLU(inplace=True),nn.Dropout(p=0.5)]\n",
    "        classifier += [nn.Linear(64,64),nn.ReLU(inplace=True),nn.Dropout(p=0.5)]\n",
    "        classifier += [nn.Linear(64,2)]\n",
    "        return nn.Sequential(*classifier)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "#         out = self.linear1(out)\n",
    "#         out = self.linear2(out)\n",
    "#         out = self.linear3(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        \"\"\"\n",
    "        cfg: a list define layers this layer contains\n",
    "            'M': MaxPool, number: Conv2d(out_channels=number) -> BN -> ReLU\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "            \n",
    "#         layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Crack_cfg = {\n",
    "    'Crack11':[16,16,'M',32,32,'M']\n",
    "}\n",
    "\n",
    "model_t = Crack(Crack_cfg['Crack11']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Crack:\n\tMissing key(s) in state_dict: \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.3.weight\", \"classifier.3.bias\", \"classifier.6.weight\", \"classifier.6.bias\". \n\tUnexpected key(s) in state_dict: \"linear1.weight\", \"linear1.bias\", \"linear2.weight\", \"linear2.bias\", \"linear3.weight\", \"linear3.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bd9f6e25c2a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# model = torch.load('D:/大三下/人工神经网络/best.pt')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./crack0.9373 REC_ 0.9331.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mmodel_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mmodel_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m    767\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m--> 769\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m    770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_named_members\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Crack:\n\tMissing key(s) in state_dict: \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.3.weight\", \"classifier.3.bias\", \"classifier.6.weight\", \"classifier.6.bias\". \n\tUnexpected key(s) in state_dict: \"linear1.weight\", \"linear1.bias\", \"linear2.weight\", \"linear2.bias\", \"linear3.weight\", \"linear3.bias\". "
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import pickle\n",
    "import torch\n",
    "# pickle.load = partial(pickle.load, encoding=\"latin1\")\n",
    "# pickle.Unpickler = partial(pickle.Unpickler, encoding=\"latin1\")\n",
    "# model = torch.load('D:\\\\大三下\\\\人工神经网络\\\\crack.pt', map_location=lambda storage, loc: storage, pickle_module=pickle)\n",
    "\n",
    "\n",
    "# model = torch.load('D:/大三下/人工神经网络/best.pt')\n",
    "model = torch.load('./crack0.9373REC_0.9331.pt')\n",
    "model_t.load_state_dict(model)\n",
    "\n",
    "model_t.eval()\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    " 1.需要将图片reshape成,长和宽都是27的倍数\n",
    " 2.切割图片\n",
    " 3.将这批图片处理成4元tuple，正则化到[-1,1]区间\n",
    " 4.将图片用模型检验\n",
    " 5.print实验结果 \n",
    " 6.将tensor转换成图片\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from skimage import io, transform\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "test_trainsforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "def classify(img):\n",
    "    # 切分为imgNum^2份\n",
    "    width_num = img.shape[0]//(27)    \n",
    "    print(width_num)\n",
    "    height_num = img.shape[1]//(27)\n",
    "    print(height_num)\n",
    "    # 分割图片\n",
    "    img_roi = []\n",
    "    for i in range(width_num):  # [1]480*360==15*11---height\n",
    "        for j in range(height_num):  # [2]column-----------width\n",
    "            img_roi.append(img[(i * 27):((i + 1) * 27), (j * 27):((j + 1) * 27)])\n",
    "\n",
    "    # 转换图片并开始识别\n",
    "    t = img_roi[0]\n",
    "    t = to_pil(t)\n",
    "    t = test_trainsforms(t).float()\n",
    "    t = t.unsqueeze_(0)\n",
    "    # print(t.size())\n",
    "    for i in range(len(img_roi)):\n",
    "        if i != 0 :\n",
    "            te = img_roi[i]\n",
    "            te = to_pil(te)\n",
    "            te = test_trainsforms(te).float()\n",
    "            te = te.unsqueeze_(0)\n",
    "            t = torch.cat((t,te),0)\n",
    "            # print(t.size())\n",
    "    return predict_img(t)\n",
    "\n",
    "def classify_v2(img):\n",
    "    #对像素逐一采样并输出，会迭代（480-27）*（320-27）次\n",
    "    width = img.shape[0]\n",
    "    height = img.shape[1]\n",
    "    arr = []\n",
    "    for i in range(width):\n",
    "        for j in range(height):\n",
    "            if(i-14 > 0 and i + 13 < width and j - 14 > 0 and j + 13 < height):\n",
    "                t = img[i-14:i+13,j-14:j+13]\n",
    "                t = to_pil(t)\n",
    "                t = test_trainsforms(t).float()\n",
    "                t = t.unsqueeze_(0)\n",
    "                temp = predict_img(t)\n",
    "                arr.append(temp)\n",
    "        if(i % 100 == 0):\n",
    "            print(i)\n",
    "    return arr\n",
    "\n",
    "def classify_v3(img):\n",
    "    #对像素逐一采样并输出，会迭代（480-27）*（320-27）次\n",
    "    width = img.shape[0]\n",
    "    height = img.shape[1]\n",
    "    arr = []\n",
    "    t = img[0:27,0:27]\n",
    "    t = to_pil(t)\n",
    "    t = test_trainsforms(t).float()\n",
    "    t = t.unsqueeze_(0)\n",
    "    index=0 \n",
    "    for i in range(width):\n",
    "        for j in range(height):\n",
    "            if(i-14 > 0 and i + 13 < width and j - 14 > 0 and j + 13 < height):\n",
    "                index += 1\n",
    "                te = img[i-14:i+13,j-14:j+13]\n",
    "                te = to_pil(te)\n",
    "                te = test_trainsforms(te).float()\n",
    "                te = te.unsqueeze_(0)\n",
    "                if(index % 256 == 0):\n",
    "                    temp = predict_img(t)\n",
    "                    t = te\n",
    "                    arr.append(temp)\n",
    "#                     print(index // 320)\n",
    "                else:\n",
    "                    t = torch.cat((t,te),0)\n",
    "        if(i%100 == 0):\n",
    "            print(i)\n",
    "    return arr\n",
    "# \t# print(st)\n",
    "# \t# 对目标进行定位\n",
    "# \tloc = []\n",
    "# \tfor i in st:\n",
    "# \t    x = i % imgNum\n",
    "# \t    y = i // imgNum\n",
    "# \t    loc.append([x,y])\n",
    "# \t# print(loc)\n",
    "# \t# 在图片上标记目标\n",
    "# \tfor i in loc:\n",
    "# \t    cv2.rectangle(img, (i[0]*imw,i[1]*imh), (i[0]*imw+imw,i[1]*imh+imh), (255,0,0), 3)\n",
    "# \t# cv2.rectangle(img, 左上角, 右下角, （r，g，b）, 粗细（1，2，3，，）)\n",
    "# \t# 返回已画好的图片\n",
    "# \treturn img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img(inputs):\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model_t(inputs)\n",
    "        imgNum = inputs.size()[0]\n",
    "        _,preds = torch.max(outputs,1)\n",
    "        std = []\n",
    "#         print(preds)\n",
    "        return preds\n",
    "#         for j in range(inputs.size()[0]):\n",
    "#             # ax = plt.subplot(imgNum//2,2,j+1)\n",
    "#             # ax.axis('off')\n",
    "#             # ax.set_title('predicted: {}'.format(classname[preds[j]]))\n",
    "#             if preds[j] == 0:\n",
    "#                 std.append(j)\n",
    "#             # imshow(inputs.cpu().data[j])\n",
    "#         return std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.5,0.5,0.5])\n",
    "    std = np.array([0.5,0.5,0.5])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "#     plt.imshow(inp)\n",
    "#     if title is not None:\n",
    "#         plt.title(title)\n",
    "#     plt.pause(0.001)  # pause a bit so that plots are updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "file = \"I:/1裂缝检测/CrackForest-dataset/image/075.jpg\"\n",
    "\n",
    "img = io.imread(file)\n",
    "#     print(classify(img))\n",
    "inp = classify_v2(img)\n",
    "# print(inp.shape)\n",
    "# inp = inp.numpy();\n",
    "# inp = inp.reshape(187,5,5)\n",
    "# print(inp.shape)\n",
    "\n",
    "#此处需要将其转化成灰度图\n",
    "    \n",
    "# mean = np.array([0.5,0.5,0.5,1])\n",
    "# std = np.array([0.5,0.5,0.5,0])\n",
    "# inp = std * inp[1] + mean\n",
    "# inp = np.clip(inp[1], 0, 1)\n",
    "    \n",
    "#     new_img_PIL = transforms.ToPILImage()(out).convert('RGB')\n",
    "#     new_img_PIL.show() # 处理后的PIL图片\n",
    "#     output = classify(img)\n",
    "#     print(output)\n",
    "#     imshow(output[1]);\n",
    "\n",
    "# \tfiles = os.listdir(path)\n",
    "# \tfile_paths=[]#构造一个存放图片的列表数据结构\n",
    "# \tfor file in files:\n",
    "# \t    file_path= path +\"\\\\\" + file\n",
    "# \t    file_paths.append(file_path)\n",
    "# \tst = '../testdata/img8/'\n",
    "# \tfor i in range(len(file_paths)):\n",
    "# \t\timg = io.imread(file_paths[i])\n",
    "# \t\timg = classify(img,imgNum) # 将图片分成imgNum*imgNum份进行识别\n",
    "# \t\tplt.imshow(img)\n",
    "# \t\timg_name = st + str(i) + '.jpg'\n",
    "# \t\tprint(img_name)\n",
    "# \t\tcv2.imwrite(img_name,img)\n",
    "# \t\tif(i>100):\n",
    "# \t\t\tbreak;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131984\n"
     ]
    }
   ],
   "source": [
    "# classify_v2 有效果\n",
    "temp = np.array(inp)\n",
    "print(temp.size)\n",
    "\n",
    "# classify_v3,没啥效果\n",
    "# # print(inp)\n",
    "# # arr = inp[0]\n",
    "# # for x in range(len(inp)-1):\n",
    "# #     arr=torch.cat((arr,inp[x+1]),0)\n",
    "# # temp = np.array(arr)\n",
    "# # print(temp.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(293, 453)\n"
     ]
    }
   ],
   "source": [
    "temp.resize(320-27,480-27)\n",
    "print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " ...\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('input.csv', temp, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = temp\n",
    "# t = (t - np.min(t))/(np.max(t)-np.min(t))\n",
    "# print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # t = temp\n",
    "# t[t> 0.6] = 1\n",
    "# t[t <= 0.4]= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23833c0d828>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(temp, cmap=plt.cm.gray, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array(inp[1])\n",
    "t[t > 0] = 1\n",
    "t[t<=0] = 0\n",
    "# t = inp.reshape(11*5,17*5)\n",
    "t = np.empty([11*5,17*5],dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "for i in range(10):\n",
    "    for j in range(16):\n",
    "        t[i*5:i*5+5,j*5:(j+1)*5] = inp[16*i+j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(t[0:5,5:10])\n",
    "# print(inp[117])\n",
    "# print(inp[116])\n",
    "temp = np.array(inp)\n",
    "temp.resize(11*5,17*5)\n",
    "print(temp.shape)\n",
    "np.savetxt('t.csv', t, delimiter = ',')\n",
    "np.savetxt('inp.csv', temp, delimiter = ',')\n",
    "print(inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = temp;\n",
    "# temp = ((temp-np.min(img)))/(np.max(img)-np.min(img))\n",
    "plt.imshow(temp, cmap=plt.cm.gray, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = t;\n",
    "t = ((t-np.min(img)))/(np.max(img)-np.min(img))\n",
    "print(t)\n",
    "np.savetxt('new2.csv', t, delimiter = ',')\n",
    "# print(t.shape)\n",
    "# print(np.argwhere(t > 0.5).size)\n",
    "\n",
    "# plt.imshow(t, cmap=plt.cm.gray, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = np.array(t)\n",
    "# temp[temp < 1] = 0\n",
    "plt.imshow(temp, cmap=plt.cm.gray, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
