{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "class Crack(nn.Module):\n",
    "    def __init__(self, Crack_cfg):\n",
    "        super(Crack, self).__init__()\n",
    "        self.features = self._make_layers(Crack_cfg)\n",
    "        self.linear1 = nn.Linear(32*6*6,64)\n",
    "        self.linear2 = nn.Linear(64,64)\n",
    "        self.linear3 = nn.Linear(64,2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "#         print(out.size())\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.linear3(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        \"\"\"\n",
    "        cfg: a list define layers this layer contains\n",
    "            'M': MaxPool, number: Conv2d(out_channels=number) -> BN -> ReLU\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "            \n",
    "#         layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Crack_cfg = {\n",
    "    'Crack11':[16,16,'M',32,32,'M']\n",
    "}\n",
    "\n",
    "model_t = Crack(Crack_cfg['Crack11']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Crack(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (linear1): Linear(in_features=1152, out_features=64, bias=True)\n",
       "  (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (linear3): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "import pickle\n",
    "import torch\n",
    "# pickle.load = partial(pickle.load, encoding=\"latin1\")\n",
    "# pickle.Unpickler = partial(pickle.Unpickler, encoding=\"latin1\")\n",
    "# model = torch.load('D:\\\\大三下\\\\人工神经网络\\\\crack.pt', map_location=lambda storage, loc: storage, pickle_module=pickle)\n",
    "\n",
    "\n",
    "# model = torch.load('D:/大三下/人工神经网络/best.pt')\n",
    "model = torch.load('./crack0.9373REC_0.9331.pt')\n",
    "model_t.load_state_dict(model)\n",
    "\n",
    "model_t.eval()\n",
    "device = \"cuda:1\"\n",
    "model_t.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    " 1.需要将图片reshape成,长和宽都是27的倍数\n",
    " 2.切割图片\n",
    " 3.将这批图片处理成4元tuple，正则化到[-1,1]区间\n",
    " 4.将图片用模型检验\n",
    " 5.print实验结果 \n",
    " 6.将tensor转换成图片\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from skimage import io, transform\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "test_trainsforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "def classify(img):\n",
    "    # 切分为imgNum^2份\n",
    "    width_num = img.shape[0]//(27)    \n",
    "    print(width_num)\n",
    "    height_num = img.shape[1]//(27)\n",
    "    print(height_num)\n",
    "    # 分割图片\n",
    "    img_roi = []\n",
    "    for i in range(width_num):  # [1]480*360==15*11---height\n",
    "        for j in range(height_num):  # [2]column-----------width\n",
    "            img_roi.append(img[(i * 27):((i + 1) * 27), (j * 27):((j + 1) * 27)])\n",
    "\n",
    "    # 转换图片并开始识别\n",
    "    t = img_roi[0]\n",
    "    t = to_pil(t)\n",
    "    t = test_trainsforms(t).float()\n",
    "    t = t.unsqueeze_(0)\n",
    "    # print(t.size())\n",
    "    for i in range(len(img_roi)):\n",
    "        if i != 0 :\n",
    "            te = img_roi[i]\n",
    "            te = to_pil(te)\n",
    "            te = test_trainsforms(te).float()\n",
    "            te = te.unsqueeze_(0)\n",
    "            t = torch.cat((t,te),0)\n",
    "            # print(t.size())\n",
    "    return predict_img(t)\n",
    "\n",
    "def classify_v2(img):\n",
    "    #对像素逐一采样并输出，会迭代（480-27）*（320-27）次\n",
    "    width = img.shape[0]\n",
    "    height = img.shape[1]\n",
    "    arr = []\n",
    "    for i in range(14,width-13):\n",
    "        for j in range(14,height-13):\n",
    "#             if(i-14 > 0 and i + 13 < width and j - 14 > 0 and j + 13 < height):\n",
    "            t = img[i-14:i+13,j-14:j+13]\n",
    "            t = to_pil(t)\n",
    "            t = test_trainsforms(t).float()\n",
    "            t = t.unsqueeze_(0)\n",
    "            temp = predict_img(t)\n",
    "#                 print(np.array(temp).shape)\n",
    "#                 if(i % 27 == 0):\n",
    "#                 print(temp)\n",
    "            arr.append(temp.cpu())\n",
    "        if(i % 100 == 0):\n",
    "            print(i)\n",
    "    return arr\n",
    "# \t# print(st)\n",
    "# \t# 对目标进行定位\n",
    "# \tloc = []\n",
    "# \tfor i in st:\n",
    "# \t    x = i % imgNum\n",
    "# \t    y = i // imgNum\n",
    "# \t    loc.append([x,y])\n",
    "# \t# print(loc)\n",
    "# \t# 在图片上标记目标\n",
    "# \tfor i in loc:\n",
    "# \t    cv2.rectangle(img, (i[0]*imw,i[1]*imh), (i[0]*imw+imw,i[1]*imh+imh), (255,0,0), 3)\n",
    "# \t# cv2.rectangle(img, 左上角, 右下角, （r，g，b）, 粗细（1，2，3，，）)\n",
    "# \t# 返回已画好的图片\n",
    "# \treturn img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img(inputs):\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model_t(inputs)\n",
    "        imgNum = inputs.size()[0]\n",
    "        _,preds = torch.max(outputs,1)\n",
    "        std = []\n",
    "#         print(preds)\n",
    "        return preds\n",
    "#         for j in range(inputs.size()[0]):\n",
    "#             # ax = plt.subplot(imgNum//2,2,j+1)\n",
    "#             # ax.axis('off')\n",
    "#             # ax.set_title('predicted: {}'.format(classname[preds[j]]))\n",
    "#             if preds[j] == 0:\n",
    "#                 std.append(j)\n",
    "#             # imshow(inputs.cpu().data[j])\n",
    "#         return std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.5,0.5,0.5])\n",
    "    std = np.array([0.5,0.5,0.5])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "#     plt.imshow(inp)\n",
    "#     if title is not None:\n",
    "#         plt.title(title)\n",
    "#     plt.pause(0.001)  # pause a bit so that plots are updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "074.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "075.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "076.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "077.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "078.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "079.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "080.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "081.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "082.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "083.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "084.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "085.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "086.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "087.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "088.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "089.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "090.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "091.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "092.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "093.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "094.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "095.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "096.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "097.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "098.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "099.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "100.jpg\n",
      "100\n",
      "200\n",
      "300\n",
      "101.jpg\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# path = \"./image/\"\n",
    "path = \"./image/\"\n",
    "savePath = \"./img/\"\n",
    "\n",
    "# file_list = os.listdir(path)\n",
    "# img_list = [file_name for file_name in file_list if file_name.endswith(\".jpg\")]\n",
    "# print(\"find mat file : \", img_list)\n",
    "    \n",
    "\n",
    "# file = \"I:/1裂缝检测/CrackForest-dataset/image/090.jpg\"\n",
    "for i in range(74,118):\n",
    "    img_name = \"{:0>3d}.jpg\".format(i)\n",
    "    print(img_name)\n",
    "    img = io.imread(path + img_name)\n",
    "    img2 = cv2.copyMakeBorder(img,14,13,14,13,cv2.BORDER_ISOLATED)\n",
    "    inp = classify_v2(img2)\n",
    "    temp = np.array(inp)\n",
    "    temp.resize(320,480)\n",
    "    temp = temp * 256\n",
    "#     print(temp)\n",
    "    cv2.imwrite(savePath + img_name,temp)  \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 480, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from skimage import io, transform\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from skimage import io, transform\n",
    "path=\"./image/\"\n",
    "im = io.imread(path+\"001.jpg\")\n",
    "temp = np.array(im)\n",
    "print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 320, 480)\n",
      "(320, 480, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3bcd2a30b8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = np.ones(320*480)\n",
    "t = t.reshape(320,480)\n",
    "t = t*256\n",
    "temp = []\n",
    "temp.append(t)\n",
    "t = np.zeros(320*480)\n",
    "t = t.reshape(320,480)\n",
    "t = t*256\n",
    "temp.append(t)\n",
    "\n",
    "temp.append(t)\n",
    "temp = np.array(temp)\n",
    "print(np.array(temp).shape)\n",
    "temp =temp.transpose((1, 2, 0))\n",
    "print(temp.shape)\n",
    "plt.imshow(temp, cmap=plt.cm.gray, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resize该矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.array(inp)\n",
    "temp.resize(320,480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存灰度图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp * 256\n",
    "cv2.imwrite(\"./22.jpg\",temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显示灰度图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(temp, cmap=plt.cm.gray, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显示原图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = io.imread(file)\n",
    "temp = np.array(img)\n",
    "plt.imshow(temp, cmap=plt.cm.gray, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
